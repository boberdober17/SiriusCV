{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CITYSCAPES_DATASET=/home/rvygon/data/\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, glob, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import skimage\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from tensorflow.python.keras.models import *\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "# from keras import backend as keras\n",
    "import time\n",
    "import functools\n",
    "from eval import *\n",
    "from ShowColors import *\n",
    "from ImportUtil import *\n",
    "%env CITYSCAPES_DATASET = /home/rvygon/data/\n",
    "from tensorflow.metrics import *\n",
    "batch_size = 2\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_image(image, label, width_shift_range, height_shift_range):\n",
    "    \"\"\"horisontal or vertical shift\"\"\"\n",
    "    if width_shift_range or height_shift_range:\n",
    "        if width_shift_range:\n",
    "            width_shift_range = tf.random_uniform([], -width_shift_range * img_shape[1], width_shift_range * img_shape[1])\n",
    "        if height_shift_range:\n",
    "            height_shift_range = tf.random_uniform([], -height_shift_range * img_shape[0], height_shift_range * img_shape[0])\n",
    "        image = tf.contrib.image.translate(image, [width_shift_range, height_shift_range])\n",
    "        label = tf.contrib.image.translate(label, [width_shift_range, height_shift_range])\n",
    "    return image, label\n",
    "\n",
    "def flip_img(horizontal_flip, image, label):\n",
    "    if horizontal_flip:\n",
    "        flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
    "        image, label = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                   lambda: (tf.image.flip_left_right(image), tf.image.flip_left_right(label)),\n",
    "                                   lambda: (image, label))\n",
    "    return image, label            \n",
    "\n",
    "def crop_img(crop_rate, image, label):\n",
    "    if crop_rate is not None:\n",
    "        image = tf.image.resize_images(tf.image.central_crop(image, crop_rate), (img_shape[0], img_shape[1]), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        label = tf.image.resize_images(tf.image.central_crop(label, crop_rate), (img_shape[0], img_shape[1]), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image, label\n",
    "\n",
    "def _augment(image,\n",
    "             label,\n",
    "             hue_delta=0,\n",
    "             horisontal_flip=False,\n",
    "             width_shift_range=0,\n",
    "             height_shift_range=0,\n",
    "             crop_rate=None):\n",
    "    if hue_delta:\n",
    "        image = tf.image.random_hue(image, hue_delta)\n",
    "    image, label = flip_img(horisontal_flip, image, label)\n",
    "    image, label = shifted_image(image, label, width_shift_range, height_shift_range)\n",
    "    image, label = crop_img(crop_rate, image, label)\n",
    "    return image, label\n",
    "def to_tensor(image, label):\n",
    "    return image, label\n",
    "\n",
    "def get_dataset(images, labels,\n",
    "                preproc_fn=functools.partial(_augment),\n",
    "                threads=5,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True):\n",
    "    num_x = len(images)\n",
    "    features_placeholder = tf.placeholder(images.dtype, images.shape)\n",
    "    labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "   \n",
    "    #dataset = dataset.map(to_tensor, num_parallel_calls=threads)\n",
    "   # dataset = dataset.map(preproc_fn)\n",
    "#     if shuffle:\n",
    "#         dataset = dataset.shuffle(num_x)\n",
    "    \n",
    "   \n",
    "   \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run this cell once\n",
    "#%run  cityscapesscripts/preparation/createTrainIdLabelImgs\n",
    "def upd_print(str):\n",
    "            sys.stdout.write('\\r')       \n",
    "            sys.stdout.write(str)\n",
    "            sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_shape = (256, 512, 3)\n",
    "batch_size = 1\n",
    "total_size = 2000\n",
    "val_size = 500\n",
    "scale_rate = 4\n",
    "verbose = 1\n",
    "start_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded files input -  100\n",
      "loaded files input -  200\n",
      "loaded files input -  300\n",
      "loaded files input -  400\n",
      "loaded files input -  500\n",
      "loaded files input -  600\n",
      "loaded files input -  700\n",
      "loaded files input -  800\n",
      "loaded files input -  900\n",
      "loaded files input -  1000\n",
      "loaded files input -  1100\n",
      "loaded files input -  1200\n",
      "loaded files input -  1300\n",
      "loaded files input -  1400\n",
      "loaded files input -  1500\n",
      "loaded files input -  1600\n",
      "loaded files input -  1700\n",
      "loaded files input -  1800\n",
      "loaded files input -  1900\n",
      "loaded files input -  2000\n",
      "loaded files output -  100\n",
      "loaded files output -  200\n",
      "loaded files output -  300\n",
      "loaded files output -  400\n",
      "loaded files output -  500\n",
      "loaded files output -  600\n",
      "loaded files output -  700\n",
      "loaded files output -  800\n",
      "loaded files output -  900\n",
      "loaded files output -  1000\n",
      "loaded files output -  1100\n",
      "loaded files output -  1200\n",
      "loaded files output -  1300\n",
      "loaded files output -  1400\n",
      "loaded files output -  1500\n",
      "loaded files output -  1600\n",
      "loaded files output -  1700\n",
      "loaded files output -  1800\n",
      "loaded files output -  1900\n",
      "loaded files output -  2000\n",
      "loaded files input -  100\n",
      "loaded files input -  200\n",
      "loaded files input -  300\n",
      "loaded files input -  400\n",
      "loaded files input -  500\n",
      "loaded files output -  100\n",
      "loaded files output -  200\n",
      "loaded files output -  300\n",
      "loaded files output -  400\n",
      "loaded files output -  500\n"
     ]
    }
   ],
   "source": [
    "x_train_data, y_train_data = importBatch(total_size, start_index, verbose,'train', scale_rate)\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "y_train_data = to_categorical(y_train_data)\n",
    "x_train_data = x_train_data.astype('uint8')\n",
    "y_train_data = y_train_data.astype('uint8')\n",
    "\n",
    "\n",
    "x_val_data, y_val_data, files = importBatch(val_size, start_index, verbose, 'val', scale_rate)\n",
    "y_val_data = to_categorical(y_val_data)\n",
    "x_val_data = x_val_data.astype('uint8')\n",
    "y_val_data = y_val_data.astype('uint8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#UNET https://github.com/zhixuhao/unet/blob/master/model.py\n",
    "#def IoU_metric(y_true, y_preds):\n",
    "    #print(y_true.shape)\n",
    "   # return tf.metrics.mean_iou(y_true,y_preds,20)\n",
    "def unet(pretrained_weights=None, input_size=img_shape):\n",
    "    inputs = Input(input_size)\n",
    "  \n",
    "    conv1 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)    \n",
    "    conv1 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))#drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(16, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(8, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(8, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    #conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(20, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    model = Model([inputs], [conv10])\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    if(pretrained_weights):\n",
    "        model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cfg = {\n",
    "    'hue_delta': 0.2,\n",
    "    'horisontal_flip': True,\n",
    "    'crop_rate' : 0.5\n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_cfg = {\n",
    "}\n",
    "val_preprocessing_fn = functools.partial(_augment, **val_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(x_train_data, \n",
    "                       y_train_data,\n",
    "                       preproc_fn=tr_preprocessing_fn,\n",
    "                       batch_size=1)\n",
    "val_ds = get_dataset(x_val_data,\n",
    "                      y_val_data,\n",
    "                      preproc_fn=val_preprocessing_fn,\n",
    "                      batch_size=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST #\n",
    "\"\"\"\n",
    "temp_ds = get_dataset(x_train_data,\n",
    "                     y_train_data,\n",
    "                     preproc_fn=tr_preprocessing_fn,\n",
    "                     batch_size=10,\n",
    "                     shuffle=False)\n",
    "data_aug_iter = temp_ds.make_one_shot_iterator()\n",
    "next_element = data_aug_iter.get_next()\n",
    "for i in range(10):\n",
    "    with tf.Session() as sess:\n",
    "        batch_of_imgs, batch_of_labels = sess.run(next_element)        \n",
    "        print(batch_of_imgs.shape)\n",
    "        img = batch_of_imgs[i]\n",
    "        label = batch_of_labels[i]\n",
    "        label = np.argmax(label, axis=2)        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Original\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(label)\n",
    "        plt.title(\"Masked\")\n",
    "        plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "(2000, 256, 512, 3)\n",
      "Epoch 1/50\n",
      "3000/3000 [==============================] - 273s 91ms/step - loss: 1.5892 - acc: 0.3262\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.58915, saving model to unet_tensorflow.hdf5\n",
      "Epoch 2/50\n",
      "3000/3000 [==============================] - 266s 89ms/step - loss: 1.0953 - acc: 0.4783\n",
      "\n",
      "Epoch 00002: loss improved from 1.58915 to 1.09528, saving model to unet_tensorflow.hdf5\n",
      "Epoch 3/50\n",
      "3000/3000 [==============================] - 265s 88ms/step - loss: 0.1347 - acc: 0.3501\n",
      "\n",
      "Epoch 00003: loss improved from 1.09528 to 0.13475, saving model to unet_tensorflow.hdf5\n",
      "Epoch 4/50\n",
      "3000/3000 [==============================] - 266s 89ms/step - loss: 1.1921e-07 - acc: 0.3299\n",
      "\n",
      "Epoch 00004: loss improved from 0.13475 to 0.00000, saving model to unet_tensorflow.hdf5\n",
      "Epoch 5/50\n",
      " 866/3000 [=======>......................] - ETA: 3:08 - loss: 1.1921e-07 - acc: 0.3302"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-4aa55518adb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unet_tensorflow.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1361\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m           logging.warning('Your dataset iterator ran out of data; '\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m     \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Starting training...')\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=';')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    features_placeholder = tf.placeholder(x_train_data.dtype, x_train_data.shape)\n",
    "    labels_placeholder = tf.placeholder(y_train_data.dtype, y_train_data.shape)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "    dataset = dataset.repeat().batch(batch_size)\n",
    "   \n",
    "    iterator1 = dataset.make_initializable_iterator()\n",
    "    sess.run(iterator1.initializer, feed_dict={features_placeholder: x_train_data,\n",
    "                                          labels_placeholder: y_train_data})\n",
    "    print(x_train_data.shape)\n",
    "    model = unet()#load_model('unet_tensorflow.hdf5')\n",
    "    model_checkpoint = ModelCheckpoint('unet_tensorflow.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "    \n",
    "    history = model.fit(iterator1, steps_per_epoch=3000,epochs=50,callbacks=[model_checkpoint, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = unet()\n",
    "# generator = keras_generator(1,4)\n",
    "# model_checkpoint = ModelCheckpoint('unet_tensorflow.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "# history = model.fit_generator(generator,steps_per_epoch=10,epochs=8,callbacks=[model_checkpoint])#, validation_data=(x_val,y_val),validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(256, 512)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCALE = 4\n",
    "#tic = time.time()\n",
    "#x_val, y_val, filenames = importBatch(500,0,0,'val',SCALE)\n",
    "#toc = time.time()\n",
    "#print('Load validation batch:', toc - tic)\n",
    "def eval_model(model):          \n",
    "        print(1)\n",
    "        x_pred = model.predict(x_val_data,verbose=1)        \n",
    "        new_x=np.argmax(x_pred,axis=3)\n",
    "        \n",
    "        \"\"\" UPSCALING\n",
    "        new_new_x = np.zeros((new_x.shape[0],new_x.shape[1]*SCALE,new_x.shape[2]*SCALE))\n",
    "        \n",
    "        for i in range(x_pred.shape[0]):\n",
    "            new_new_x[i]=UpscaleImg(new_x[i],SCALE,0)\n",
    "            upd_print((\"Upscaled %d images\" % i))\n",
    "        print()\n",
    "        new_new_x=new_new_x.astype(int)\"\"\"\n",
    "        \n",
    "        new_x=new_x.astype(int)        \n",
    "        \n",
    "        \"\"\"   SAVE\n",
    "        cityscapesPath = os.environ['CITYSCAPES_DATASET']        \n",
    "        for i in range(len(filenames)):\n",
    "            impath = os.path.join(cityscapesPath,'results', filenames[i].split('/')[7]+'.png')           \n",
    "            imsave(impath, new_new_x[i])\n",
    "            upd_print(\"Saved %d images\" % i)            \n",
    "        print()\n",
    "        toc = time.time()\n",
    "        print('Save files:', toc -tic) \"\"\"\n",
    "        y_val = np.argmax(y_val_data,axis=3)\n",
    "        score = eval_preds(new_x,y_val)        \n",
    "        return score\n",
    "x_pred = model.predict(x_val_data[0:1],verbose=1)  \n",
    "pred_image=np.argmax(x_pred[0],axis=2).astype(int)\n",
    "pred_image.shape\n",
    "y_img = np.argmax(y_val_data[0],axis=2).astype(int)\n",
    "y_img.shape\n",
    "#colorImage(, np.argmax(y_val_data[0],axis=2).astype(int), 'classes.txt', 'colors.txt')\n",
    "#model = load_model('unet_RVsaved1.hdf5')\n",
    "#print(eval_model(model))\n",
    "#print(y_val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
