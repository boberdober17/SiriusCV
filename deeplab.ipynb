{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CITYSCAPES_DATASET=/home/rvygon/data/\n"
     ]
    }
   ],
   "source": [
    "import os, glob, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import skimage\n",
    "from skimage.io import imread, imshow, imsave\n",
    "from tensorflow.python.keras.models import *\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.optimizers import *\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "import time\n",
    "import functools\n",
    "from eval import *\n",
    "from ShowColors import *\n",
    "from ImportUtil import *\n",
    "import random\n",
    "%env CITYSCAPES_DATASET = /home/rvygon/data/\n",
    "from tensorflow.metrics import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 4}\n",
    "    )\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "EPOCHS_PER_EVAL = 1\n",
    "BATCH_SIZE = 1\n",
    "TOTAL_SIZE = 1\n",
    "VAL_SIZE = 10\n",
    "SCALE_RATE = 4\n",
    "IMG_SHAPE = (int(1024/SCALE_RATE), int(2048/SCALE_RATE), 3)\n",
    "VERBOSE = 1\n",
    "START_INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### AUGMENTATION BLOCK\n",
    "\n",
    "def flip_img(horizontal_flip, image, label):\n",
    "    if horizontal_flip:\n",
    "        flip_prob = tf.random_uniform([], 0.0, 1.0)\n",
    "        image, label = tf.cond(tf.less(flip_prob, 0.5),\n",
    "                                   lambda: (tf.image.flip_left_right(image), tf.image.flip_left_right(label)),\n",
    "                                   lambda: (image, label))\n",
    "    return image, label            \n",
    "\n",
    "def crop_img(crop_rate, image, label):\n",
    "    if crop_rate is not None:\n",
    "        image = tf.image.resize_images(tf.image.central_crop(image, crop_rate), (IMG_SHAPE[0], IMG_SHAPE[1]), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "        label = tf.image.resize_images(tf.image.central_crop(label, crop_rate), (IMG_SHAPE[0], IMG_SHAPE[1]), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return image, label\n",
    "\n",
    "def _augment(image,\n",
    "             label,\n",
    "             hue_delta=0,\n",
    "             horisontal_flip=False,\n",
    "             width_shift_range=0,\n",
    "             height_shift_range=0,\n",
    "             crop_rate=0.25):\n",
    "    if hue_delta:\n",
    "        image = tf.image.random_hue(image, hue_delta)\n",
    "    image, label = flip_img(horisontal_flip, image, label)   \n",
    "    image, label = crop_img(crop_rate, image, label)\n",
    "    return image, label\n",
    "def to_tensor(image, label):\n",
    "    return image, label\n",
    "\n",
    "tr_cfg = {\n",
    "    'hue_delta': 0.05,\n",
    "    'horisontal_flip': True,\n",
    "    'crop_rate' : 0.25\n",
    "}\n",
    "tr_preprocessing_fn = functools.partial(_augment, **tr_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run this cell once\n",
    "#%run  cityscapesscripts/preparation/createTrainIdLabelImgs\n",
    "def upd_print(str):\n",
    "    sys.stdout.write('\\r')       \n",
    "    sys.stdout.write(str)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    alpha = 0.5\n",
    "    beta  = 0.5\n",
    "    \n",
    "    ones = K.ones(K.shape(y_true))\n",
    "    p0 = y_pred      # proba that pixels are class i\n",
    "    p1 = ones - y_pred # proba that pixels are not class i\n",
    "    g0 = y_true\n",
    "    g1 = ones - y_true\n",
    "    \n",
    "    num = K.sum(p0 * g0, (0, 1, 2))\n",
    "    den = num + alpha * K.sum(p0 * g1, (0, 1, 2)) + beta * K.sum(p1 * g0, (0, 1, 2)) + 1e-8\n",
    "    \n",
    "    T = K.sum(num / den) # when summing over classes, T has dynamic range [0 Ncl]\n",
    "    \n",
    "    classNumber = K.cast(K.shape(y_true)[-1], 'float32') ### equal classNumber = 20.0\n",
    "    return classNumber - T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 256, 512, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "x_train_data, y_train_data = importBatch(TOTAL_SIZE,\n",
    "                                         START_INDEX,\n",
    "                                         VERBOSE,\n",
    "                                         'train',\n",
    "                                         SCALE_RATE)\n",
    "#y_train_data = to_categorical(y_train_data)\n",
    "x_train_data = x_train_data.astype('float32')\n",
    "y_train_data = y_train_data.astype('int32')\n",
    "x_val_data, y_val_data, files = importBatch(VAL_SIZE,\n",
    "                                            START_INDEX,\n",
    "                                            VERBOSE,\n",
    "                                            'val',\n",
    "                                          SCALE_RATE)\n",
    "y_train_data=np.expand_dims(y_train_data,axis=3)\n",
    "y_val_data = to_categorical(y_val_data)\n",
    "x_val_data = x_val_data.astype('float32')\n",
    "y_val_data = y_val_data.astype('float32')\n",
    "y_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DeepLab v3 models based on slim library.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "from tensorflow.contrib.slim.python.slim.nets import resnet_utils\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug\n",
    "\n",
    "\n",
    "\n",
    "_NUM_CLASSES = 20\n",
    "_HEIGHT = 256\n",
    "_WIDTH = 512\n",
    "_DEPTH = 3\n",
    "_MIN_SCALE = 0.5\n",
    "_MAX_SCALE = 2.0\n",
    "_IGNORE_LABEL = 255\n",
    "\n",
    "_POWER = 0.9\n",
    "_MOMENTUM = 0.9\n",
    "\n",
    "_BATCH_NORM_DECAY = 0.9997\n",
    "\n",
    "_NUM_IMAGES = {\n",
    "    'train': 10582,\n",
    "    'validation': 1449,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "_BATCH_NORM_DECAY = 0.9997\n",
    "_WEIGHT_DECAY = 5e-4\n",
    "\n",
    "\n",
    "def atrous_spatial_pyramid_pooling(inputs, output_stride, batch_norm_decay, is_training, depth=256):\n",
    "  with tf.device('/GPU:2'):\n",
    "      \"\"\"Atrous Spatial Pyramid Pooling.\n",
    "\n",
    "      Args:\n",
    "        inputs: A tensor of size [batch, height, width, channels].\n",
    "        output_stride: The ResNet unit's stride. Determines the rates for atrous convolution.\n",
    "          the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n",
    "        batch_norm_decay: The moving average decay when estimating layer activation\n",
    "          statistics in batch normalization.\n",
    "        is_training: A boolean denoting whether the input is for training.\n",
    "        depth: The depth of the ResNet unit output.\n",
    "\n",
    "      Returns:\n",
    "        The atrous spatial pyramid pooling output.\n",
    "      \"\"\"\n",
    "      with tf.variable_scope(\"aspp\"):\n",
    "        if output_stride not in [8, 16]:\n",
    "          raise ValueError('output_stride must be either 8 or 16.')\n",
    "\n",
    "        atrous_rates = [6, 12, 18]\n",
    "        if output_stride == 8:\n",
    "          atrous_rates = [2*rate for rate in atrous_rates]\n",
    "\n",
    "        with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n",
    "          with arg_scope([layers.batch_norm], is_training=is_training):\n",
    "            inputs_size = tf.shape(inputs)[1:3]\n",
    "            # (a) one 1x1 convolution and three 3x3 convolutions with rates = (6, 12, 18) when output stride = 16.\n",
    "            # the rates are doubled when output stride = 8.\n",
    "            conv_1x1 = layers_lib.conv2d(inputs, depth, [1, 1], stride=1, scope=\"conv_1x1\")\n",
    "            conv_3x3_1 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[0], scope='conv_3x3_1')\n",
    "            conv_3x3_2 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[1], scope='conv_3x3_2')\n",
    "            conv_3x3_3 = resnet_utils.conv2d_same(inputs, depth, 3, stride=1, rate=atrous_rates[2], scope='conv_3x3_3')\n",
    "\n",
    "            # (b) the image-level features\n",
    "            with tf.variable_scope(\"image_level_features\"):\n",
    "              # global average pooling\n",
    "              image_level_features = tf.reduce_mean(inputs, [1, 2], name='global_average_pooling', keepdims=True)\n",
    "              # 1x1 convolution with 256 filters( and batch normalization)\n",
    "              image_level_features = layers_lib.conv2d(image_level_features, depth, [1, 1], stride=1, scope='conv_1x1')\n",
    "              # bilinearly upsample features\n",
    "              image_level_features = tf.image.resize_bilinear(image_level_features, inputs_size, name='upsample')\n",
    "\n",
    "            net = tf.concat([conv_1x1, conv_3x3_1, conv_3x3_2, conv_3x3_3, image_level_features], axis=3, name='concat')\n",
    "            net = layers_lib.conv2d(net, depth, [1, 1], stride=1, scope='conv_1x1_concat')\n",
    "\n",
    "            return net\n",
    "\n",
    "\n",
    "def deeplab_v3_generator(num_classes,\n",
    "                         output_stride,\n",
    "                         base_architecture,\n",
    "                         pre_trained_model,\n",
    "                         batch_norm_decay,\n",
    "                         data_format='channels_last'):\n",
    "  \n",
    "      \"\"\"Generator for DeepLab v3 models.\n",
    "\n",
    "      Args:\n",
    "        num_classes: The number of possible classes for image classification.\n",
    "        output_stride: The ResNet unit's stride. Determines the rates for atrous convolution.\n",
    "          the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n",
    "        base_architecture: The architecture of base Resnet building block.\n",
    "        pre_trained_model: The path to the directory that contains pre-trained models.\n",
    "        batch_norm_decay: The moving average decay when estimating layer activation\n",
    "          statistics in batch normalization.\n",
    "        data_format: The input format ('channels_last', 'channels_first', or None).\n",
    "          If set to None, the format is dependent on whether a GPU is available.\n",
    "          Only 'channels_last' is supported currently.\n",
    "\n",
    "      Returns:\n",
    "        The model function that takes in `inputs` and `is_training` and\n",
    "        returns the output tensor of the DeepLab v3 model.\n",
    "      \"\"\"\n",
    "      if data_format is None:\n",
    "        # data_format = (\n",
    "        #     'channels_first' if tf.test.is_built_with_cuda() else 'channels_last')\n",
    "        pass\n",
    "\n",
    "      if batch_norm_decay is None:\n",
    "        batch_norm_decay = _BATCH_NORM_DECAY\n",
    "\n",
    "      if base_architecture not in ['resnet_v2_50', 'resnet_v2_101']:\n",
    "        raise ValueError(\"'base_architrecture' must be either 'resnet_v2_50' or 'resnet_v2_101'.\")\n",
    "\n",
    "      if base_architecture == 'resnet_v2_50':\n",
    "        base_model = resnet_v2.resnet_v2_50\n",
    "      else:\n",
    "        base_model = resnet_v2.resnet_v2_101\n",
    "\n",
    "      def model(inputs, is_training):\n",
    "        \"\"\"Constructs the ResNet model given the inputs.\"\"\"\n",
    "        if data_format == 'channels_first':\n",
    "          # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n",
    "          # This provides a large performance boost on GPU. See\n",
    "          # https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "          inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "        # tf.logging.info('net shape: {}'.format(inputs.shape))\n",
    "\n",
    "        with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n",
    "          logits, end_points = base_model(inputs,\n",
    "                                          num_classes=None,\n",
    "                                          is_training=is_training,\n",
    "                                          global_pool=False,\n",
    "                                          output_stride=output_stride)\n",
    "\n",
    "        if is_training:\n",
    "          exclude = [base_architecture + '/logits', 'global_step']\n",
    "          variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "          tf.train.init_from_checkpoint(pre_trained_model,\n",
    "                                        {v.name.split(':')[0]: v for v in variables_to_restore})\n",
    "\n",
    "        inputs_size = tf.shape(inputs)[1:3]\n",
    "        net = end_points[base_architecture + '/block4']\n",
    "        net = atrous_spatial_pyramid_pooling(net, output_stride, batch_norm_decay, is_training)\n",
    "        with tf.variable_scope(\"upsampling_logits\"):\n",
    "          net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope='conv_1x1')\n",
    "          logits = tf.image.resize_bilinear(net, inputs_size, name='upsample')\n",
    "\n",
    "        return logits\n",
    "\n",
    "      return model\n",
    "\n",
    "def deeplabv3_model_fn(features, labels, mode, params):\n",
    "  images = tf.cast(features,tf.uint8)\n",
    "  #images = tf.cast(\n",
    "  #    tf.map_fn(preprocessing.mean_image_addition, features),\n",
    "  #    tf.uint8)\n",
    "\n",
    "  network = deeplab_v3_generator(params['num_classes'],\n",
    "                                 params['output_stride'],\n",
    "                                 params['base_architecture'],\n",
    "                                 params['pre_trained_model'],\n",
    "                                 params['batch_norm_decay'])\n",
    "    \n",
    "    \n",
    "  logits = network(features, mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "  pred_classes = tf.expand_dims(tf.argmax(logits, axis=3, output_type=tf.int32), axis=3)\n",
    "\n",
    "  pred_decoded_labels = tf.py_func(preprocessing.decode_labels,\n",
    "                                   [pred_classes, params['batch_size'], params['num_classes']],\n",
    "                                   tf.uint8)\n",
    "\n",
    "  predictions = {\n",
    "      'classes': pred_classes,\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor'),\n",
    "      'decoded_labels': pred_decoded_labels,      \n",
    "  }\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Delete 'decoded_labels' from predictions because custom functions produce error when used with saved_model\n",
    "    predictions_without_decoded_labels = predictions.copy()\n",
    "    del predictions_without_decoded_labels['decoded_labels']\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        export_outputs={\n",
    "            'preds': tf.estimator.export.PredictOutput(\n",
    "                predictions_without_decoded_labels)\n",
    "        })\n",
    "\n",
    "  gt_decoded_labels = tf.py_func(preprocessing.decode_labels,\n",
    "                                 [labels, params['batch_size'], params['num_classes']], tf.uint8)\n",
    "\n",
    "  labels = tf.squeeze(labels, axis=3)  # reduce the channel dimension.\n",
    "\n",
    "  logits_by_num_classes = tf.reshape(logits, [-1, params['num_classes']])\n",
    "  labels_flat = tf.reshape(labels, [-1, ])\n",
    "\n",
    "  valid_indices = tf.to_int32(labels_flat <= params['num_classes'] - 1)\n",
    "  valid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\n",
    "  valid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\n",
    "\n",
    "  preds_flat = tf.reshape(pred_classes, [-1, ])\n",
    "  valid_preds = tf.dynamic_partition(preds_flat, valid_indices, num_partitions=2)[1]\n",
    "  confusion_matrix = tf.confusion_matrix(valid_labels, valid_preds, num_classes=params['num_classes'])\n",
    "\n",
    "  predictions['valid_preds'] = valid_preds\n",
    "  predictions['valid_labels'] = valid_labels\n",
    "  predictions['confusion_matrix'] = confusion_matrix\n",
    "\n",
    "  cross_entropy = tf.losses.sparse_softmax_cross_entropy(\n",
    "      logits=valid_logits, labels=valid_labels)\n",
    "\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  if not params['freeze_batch_norm']:\n",
    "    train_var_list = [v for v in tf.trainable_variables()]\n",
    "  else:\n",
    "    train_var_list = [v for v in tf.trainable_variables()\n",
    "                      if 'beta' not in v.name and 'gamma' not in v.name]\n",
    "\n",
    "  # Add weight decay to the loss.\n",
    "  with tf.variable_scope(\"total_loss\"):\n",
    "    loss = cross_entropy + params.get('weight_decay', _WEIGHT_DECAY) * tf.add_n(\n",
    "        [tf.nn.l2_loss(v) for v in train_var_list])\n",
    "  # loss = tf.losses.get_total_loss()  # obtain the regularization losses as well\n",
    "\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    tf.summary.image('images', images)\n",
    "    tf.summary.image('valid_labels',tf.reshape(tf.cast(valid_labels,tf.uint8),(1,256,512,1)))\n",
    "    tf.summary.image('valid_preds',tf.reshape(tf.cast(valid_preds,tf.uint8),(1,256,512,1)))\n",
    "                     # Concatenate row-wise.\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    if params['learning_rate_policy'] == 'piecewise':\n",
    "      # Scale the learning rate linearly with the batch size. When the batch size\n",
    "      # is 128, the learning rate should be 0.1.\n",
    "      initial_learning_rate = 0.1 * params['batch_size'] / 128\n",
    "      batches_per_epoch = params['num_train'] / params['batch_size']\n",
    "      # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n",
    "      boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n",
    "      values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n",
    "      learning_rate = tf.train.piecewise_constant(\n",
    "          tf.cast(global_step, tf.int32), boundaries, values)\n",
    "    elif params['learning_rate_policy'] == 'poly':\n",
    "      learning_rate = tf.train.polynomial_decay(\n",
    "          params['initial_learning_rate'],\n",
    "          tf.cast(global_step, tf.int32) - params['initial_global_step'],\n",
    "          params['max_iter'], params['end_learning_rate'], power=params['power'])\n",
    "    else:\n",
    "      raise ValueError('Learning rate policy must be \"piecewise\" or \"poly\"')\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=params['momentum'])\n",
    "\n",
    "    # Batch norm requires update ops to be added as a dependency to the train_op\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(loss, global_step, var_list=train_var_list)\n",
    "  else:\n",
    "    train_op = None\n",
    "\n",
    "  accuracy = tf.metrics.accuracy(\n",
    "      valid_labels, valid_preds)\n",
    "  mean_iou = tf.metrics.mean_iou(valid_labels, valid_preds, params['num_classes'])\n",
    "  metrics = {'px_accuracy': accuracy, 'mean_iou': mean_iou}\n",
    "\n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_px_accuracy')\n",
    "  tf.summary.scalar('train_px_accuracy', accuracy[1])\n",
    "\n",
    "  def compute_mean_iou(total_cm, name='mean_iou'):\n",
    "    \"\"\"Compute the mean intersection-over-union via the confusion matrix.\"\"\"\n",
    "    sum_over_row = tf.to_float(tf.reduce_sum(total_cm, 0))\n",
    "    sum_over_col = tf.to_float(tf.reduce_sum(total_cm, 1))\n",
    "    cm_diag = tf.to_float(tf.diag_part(total_cm))\n",
    "    denominator = sum_over_row + sum_over_col - cm_diag\n",
    "\n",
    "    # The mean is only computed over classes that appear in the\n",
    "    # label or prediction tensor. If the denominator is 0, we need to\n",
    "    # ignore the class.\n",
    "    num_valid_entries = tf.reduce_sum(tf.cast(\n",
    "        tf.not_equal(denominator, 0), dtype=tf.float32))\n",
    "\n",
    "    # If the value of the denominator is 0, set it to 1 to avoid\n",
    "    # zero division.\n",
    "    denominator = tf.where(\n",
    "        tf.greater(denominator, 0),\n",
    "        denominator,\n",
    "        tf.ones_like(denominator))\n",
    "    iou = tf.div(cm_diag, denominator)\n",
    "\n",
    "    for i in range(params['num_classes']):\n",
    "      tf.identity(iou[i], name='train_iou_class{}'.format(i))\n",
    "      tf.summary.scalar('train_iou_class{}'.format(i), iou[i])\n",
    "\n",
    "    # If the number of valid entries is 0 (no classes) we return 0.\n",
    "    result = tf.where(\n",
    "        tf.greater(num_valid_entries, 0),\n",
    "        tf.reduce_sum(iou, name=name) / num_valid_entries,\n",
    "        0)\n",
    "    return result\n",
    "\n",
    "  train_mean_iou = compute_mean_iou(mean_iou[1])\n",
    "\n",
    "  tf.identity(train_mean_iou, name='train_mean_iou')\n",
    "  tf.summary.scalar('train_mean_iou', train_mean_iou)\n",
    "\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "eval_metric_ops=metrics)\n",
    "\n",
    "def input_fn(is_training, dataset, batch_size, num_epochs=1):\n",
    "    \"\"\" Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n",
    "  Args:\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    data_dir: The directory containing the input data.\n",
    "    batch_size: The number of samples per batch.\n",
    "    num_epochs: The number of epochs to repeat the dataset.\n",
    "  Returns:\n",
    "    A tuple of images and labels.\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "    return images, labels\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'home/rvygon/SiriusCV/deeplab', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc45fe9f9b0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Start training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from home/rvygon/SiriusCV/deeplab/model.ckpt-60\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 60 into home/rvygon/SiriusCV/deeplab/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 61 into home/rvygon/SiriusCV/deeplab/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "#with tf.device(\"/gpu:2\"):\n",
    "#    tf.app.run()\n",
    "\n",
    "with tf.device('/GPU:2'):\n",
    "    session_config=tf.ConfigProto(device_count={'GPU': 4})\n",
    "    # Set up a RunConfig to only save checkpoints once per training cycle.\n",
    "    run_config = tf.estimator.RunConfig().replace(save_checkpoints_steps=1)\n",
    "    \n",
    "    model = tf.estimator.Estimator(\n",
    "        model_fn=deeplabv3_model_fn,\n",
    "        config=run_config,\n",
    "        model_dir='home/rvygon/SiriusCV/deeplab',\n",
    "        params = {\n",
    "            'output_stride': 8,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'base_architecture': 'resnet_v2_101',\n",
    "            'pre_trained_model': '/home/rvygon/data/DeepLab/resnet_v2_101.ckpt',\n",
    "            'batch_norm_decay': _BATCH_NORM_DECAY,\n",
    "            'num_classes': _NUM_CLASSES,\n",
    "            'tensorboard_images_max_outputs': 6,\n",
    "            'weight_decay': 2e-4,\n",
    "            'learning_rate_policy': 'poly',\n",
    "            'num_train': 1,\n",
    "            'initial_learning_rate': 7e-3,\n",
    "            'max_iter': 3000,\n",
    "            'end_learning_rate': 1e-6,\n",
    "            'power': _POWER,\n",
    "            'momentum': _MOMENTUM,\n",
    "            'freeze_batch_norm': False,\n",
    "            'initial_global_step': 0\n",
    "          })\n",
    "\n",
    "    for _ in range(EPOCHS // EPOCHS_PER_EVAL):\n",
    "      tensors_to_log = {\n",
    "        'learning_rate': 'learning_rate',\n",
    "        'cross_entropy': 'cross_entropy',\n",
    "        'train_px_accuracy': 'train_px_accuracy',\n",
    "        'train_mean_iou': 'train_mean_iou',\n",
    "      }\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train_data, y_train_data))\n",
    "\n",
    "\n",
    "    dataset = dataset.repeat().batch(BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "      tensors=tensors_to_log, every_n_iter=10)\n",
    "    train_hooks = [logging_hook]\n",
    "    eval_hooks = None\n",
    "\n",
    "\n",
    "    tf.logging.info(\"Start training.\")\n",
    "    model.train(\n",
    "      input_fn=lambda: input_fn(True, dataset, BATCH_SIZE, EPOCHS_PER_EVAL),\n",
    "      hooks=train_hooks,\n",
    "      steps=1  # For debug\n",
    "    )\n",
    "\n",
    "    tf.logging.info(\"Start evaluation.\")\n",
    "    # Evaluate the model and print results\n",
    "    \"\"\"eval_results = model.evaluate(\n",
    "        # Batch size must be 1 for testing because the images' size differs\n",
    "        input_fn=lambda: input_fn(False, dataset, 1),\n",
    "        hooks=eval_hooks,\n",
    "        # steps=1  # For debug\n",
    "        )\n",
    "    print(eval_results)\"\"\"\n",
    "    pred = model.predict(input_fn=lambda: input_fn(False, dataset,1))\n",
    "    imshow(np.squeeze(next(pred)['classes']))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
